# âš¡ **API Deployment â€” FastAPI, Swagger UI & Postman**

This branch introduces the **API Deployment stage** of the **MLOps Gun Detection** pipeline.  
Here, we transform the trained Faster R-CNN model into a **production-ready REST API** using **FastAPI**, enabling users to upload images and receive annotated predictions in real time.

The API provides two endpoints:

| Method | Endpoint      | Description |
| :------ | :------------- | :------------ |
| `GET`   | `/`            | Basic API health check |
| `POST`  | `/predict/`    | Upload an image and return detection results |


## ðŸ§¾ **What This Stage Includes**

* âœ… `main.py` â€” FastAPI server exposing the inference endpoint  
* âœ… Integration of pretrained **Faster R-CNN (ResNet-50 FPN)**  
* âœ… Automatic documentation via **Swagger UI**  
* âœ… Endpoint validation using **Postman**  
* âœ… Complete end-to-end local inference test flow  

This marks the transition from **model training** to **real-time model serving**, laying the groundwork for scalable inference and API-based deployment.



## ðŸ§  **Creating the API**

We start by creating a new file called `main.py` in the project root.

```bash
mlops-gun-detection/
â”œâ”€â”€ main.py                   # ðŸš€ FastAPI inference API
â”œâ”€â”€ src/                      # Core modules
â”œâ”€â”€ artifacts/                # Models, images, and labels
â””â”€â”€ img/
    â””â”€â”€ api_deployment/
````

Inside `main.py`, we implemented a **FastAPI** app that loads a pretrained **Faster R-CNN (ResNet-50 FPN)** model, processes uploaded images, and returns them annotated with bounding boxes.
The API automatically detects available GPU/CPU and performs inference using PyTorch.



## âš™ï¸ **Run the FastAPI Server**

Open a terminal in the project root and run:

```bash
uvicorn main:app --reload
```

Youâ€™ll see console output indicating the API has started:

```
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

Then, open your browser and navigate to:

ðŸ‘‰ **[http://127.0.0.1:8000/docs#/](http://127.0.0.1:8000/docs#/)**

This will open the **Swagger UI** interface, where you can interact with the API directly.



## ðŸ§ª **Testing with Swagger UI**

### 1ï¸âƒ£ Access the API Docs

Once the server is running, open:

> **[http://127.0.0.1:8000/docs#/](http://127.0.0.1:8000/docs#/)**

Youâ€™ll see the FastAPI Swagger interface:

![FastAPI Swagger UI](img/api_deployment/fastapi_ui.png)



### 2ï¸âƒ£ Test the `/predict/` Endpoint

* Expand the `POST /predict/` dropdown
* Click **â€œTry it outâ€**
* Youâ€™ll now see an upload panel under the *Request Body* section

Upload any test image from your project (e.g., from `artifacts/raw/Images/`)
Then click **â€œExecuteâ€** â€” youâ€™ll see a preview of the API response:

![FastAPI Predict Endpoint](img/api_deployment/fastapi_predict.png)

The response is an annotated image stream showing red bounding boxes for detected firearms.



## ðŸ§© **Testing with Postman**

You can also test the same endpoint using **Postman** â€” ideal for API debugging and integration testing.

### 1ï¸âƒ£ Download and Set Up Postman

Go to ðŸ‘‰ [https://www.postman.com/downloads/](https://www.postman.com/downloads/)
Install the desktop application and sign in (free account works fine).



### 2ï¸âƒ£ Create a New Request

* Open Postman and create a **New Workspace**
* Click the **â€œ+â€** tab beside *Overview* to open a new request tab
* Change the request type to **POST**
* Set the URL to:

```
http://127.0.0.1:8000/predict/
```



### 3ï¸âƒ£ Configure the Request Body

* Select the **Body** tab
* Choose **form-data**
* Under **Key**, enter `file`
* On the right, change the *Text* dropdown to **File**
* Under **Value**, upload one of your training images (e.g., from `artifacts/raw/Images/`)

Then click **Send** â€” the API will return the annotated image as the response:

![Postman Prediction Response](img/api_deployment/postman_ui_predict.png)



## ðŸ§  **Outputs**

| Output                     | Description                                                       |
| :------------------------- | :---------------------------------------------------------------- |
| **Annotated Image**        | Uploaded image returned with red bounding boxes for detected guns |
| **Swagger UI Interface**   | Auto-generated FastAPI docs for interactive testing               |
| **Postman Test Workspace** | Manual API test confirming inference functionality                |
| **main.py**                | Production-ready FastAPI app for inference deployment             |



## ðŸ—‚ï¸ **Project Structure (Delta)**

```
mlops-gun-detection/
â”œâ”€â”€ main.py                     # FastAPI inference API
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ models/                 # Trained model weights
â”‚   â””â”€â”€ raw/                    # Dataset (Images + Labels)
â”œâ”€â”€ img/
â”‚   â””â”€â”€ api_deployment/
â”‚       â”œâ”€â”€ fastapi_ui.png
â”‚       â”œâ”€â”€ fastapi_predict.png
â”‚       â””â”€â”€ postman_ui_predict.png
```

## ðŸ **Project Conclusion â€” MLOps Gun Detection**

This stage marks the completion of the **MLOps Gun Detection** project â€” a fully modular, end-to-end machine learning workflow that progresses from **data ingestion** to **real-time inference**.

Through each branch and stage, we have built a structured, reproducible system encompassing:

* âš™ï¸ **Automated data ingestion** from Kaggle via KaggleHub  
* ðŸ§¹ **Data preprocessing and validation** with custom PyTorch datasets  
* ðŸ§  **Model architecture definition** using Faster R-CNN (ResNet-50 FPN)  
* ðŸ”¥ **Training orchestration and logging** with TensorBoard  
* âš¡ **API deployment** for live inference using FastAPI, Swagger UI, and Postman  

Each stage was designed to mirror real-world MLOps workflows â€” from experimentation and tracking to reproducible pipelines and model serving.

## ðŸš€ **Next Steps & Extensions**

While this project is now complete, several natural extensions could follow:

* ðŸ³ **Containerisation** â€” Package the FastAPI app into a Docker image for deployment.  
* â˜ï¸ **Cloud Deployment** â€” Deploy the container on AWS, Azure, or GCP for scalable access.  
* ðŸ“ˆ **Monitoring & Retraining** â€” Integrate Prometheus or MLflow for production tracking and continuous model improvement.  
* ðŸ” **Model Evaluation** â€” Expand metrics (mAP, precision, recall) for deeper performance insight.  

## ðŸ§© **Summary**

This project demonstrates a full **MLOps lifecycle** applied to a real object detection problem:
> From raw data âžœ reproducible pipelines âžœ trained model âžœ deployed API.

It serves as a robust, modular foundation for more advanced computer vision systems â€” ready to be scaled, containerised, and deployed to production.

ðŸŽ¯ **End of Project â€” MLOps Gun Detection**
